---
title: "standardization"
output: rmarkdown::html_vignette
header-includes:
   - \usepackage{amsmath}
vignette: >
  %\VignetteIndexEntry{standardization}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

UNDER CONSTRUCTION
<!-- Latent Gaussian models are a popular subclass of BHMs that are characterized by the assumption of Gaussianity on the latent parameters. LGMs have become increasingly popular in recent years, mainly due to the work of \cite{RM}, which introduced the Integrated Nested Laplace approximation (INLA) method for fitting this class of models  ???.  -->
<!-- We focus here on the most common class of LGMs, in which the response is linked to the latent parameters only through a generalized linear model (see Bayesian LGMs with a univariate link function in \cite{BHM_ref}). We formally denote this class of models as Model \ref{def:model}, under some useful additional assumptions that are adopted in the vast majority of LGM applications. -->

<!-- \begin{model}[Latent Gaussian Model]\label{def:model}  -->
<!-- Let $\boldsymbol{X}=[X_1,...,X_J]$ be a set of covariates with $\boldsymbol{X}\sim \pi(\boldsymbol{x})$, and possible realizations $x\in \mathcal{X}_j,\;\forall j=1,...,J$. Let a response $Y\sim \text{Dist}(\eta,\boldsymbol{\psi})$, where $\eta$ corresponds to a  transformation of the expected value of Y given $\boldsymbol{\psi}$ and is defined as: -->
<!-- \begin{align*} -->
<!--        \eta &=\mu+\sum_{j=1}^Jf_j(X_j) -->
<!-- \end{align*} -->
<!-- Each $f_j(X_j)$ for $j=1,...,J$ is defined as:  -->
<!-- \begin{gather*} -->
<!--        f_j(X_j)=\boldsymbol{D}^T_j(X_j)\boldsymbol{u}_j\text{    where    } -->
<!--      \boldsymbol{u}_j|\sigma^2_j   \sim N_{K_j}(\boldsymbol{0},\sigma^2_j\boldsymbol{Q}_j^*) -->
<!-- \end{gather*} -->
<!-- where the basis $\boldsymbol{D}_j(X_j)$ is a column vector containing $K_j$ known functions, the precision matrix $\boldsymbol{Q}_j$ is known, and the scalar $\sigma^2_j$ is the variance parameter. -->

<!-- The parameters of the model that require prior specification are $\boldsymbol{\sigma}=[\sigma^2,...,\sigma^2_J]$, along with $\mu$ and $\boldsymbol{\psi}$. -->
<!-- \end{model} -->

<!-- Note that $\boldsymbol{Q}_j^*$ represents the generalized inverse of $\boldsymbol{Q}_j^*$, so that Model 1 also consider improper Gaussian effects, as long as the appropriate constraints are imposed (see Section \ref{sec:IGMRFs}). Moreover, if any of $\boldsymbol{Q}_j$ is not actually fixed but controlled by additional parameters (e.g. the range in Matern processes), such cases can still be part of the model class as long as the model is conditioned upon a reasonable value of such parameters and it is assumed reasonable to carry out the prior specification of $\boldsymbol{\sigma}$ independently of these additional parameters \parencite{F20}. -->

<!-- \begin{definition}[Fixed and random effects]\label{def:fixed_random} -->
<!-- Consider each effect $f_j(X_j)$ from Model 1: if the user is interested in inference about $\boldsymbol{u}_j$ (or transformations), we shall refer to the $j$ effect as "fixed"; if the user is instead interested in inference about the parameter $\sigma^2_j$ (or transformations), we shall refer to it as "random". The effects of Model 1 shall be ordered such that $\boldsymbol{\theta}=[\boldsymbol{u}_1,...,\boldsymbol{u}_L,\sigma^2_{L+1},...,\sigma^2_J]$ contains all the parameters of interest for the user, i.e. the first $L$ effects are fixed, and the last $J-L$ are random. -->
<!-- \end{definition} -->

<!-- \begin{definition}[Intuitive interpretation of $\sigma^2_j$ parameters]\label{def:int_int}\ \\ -->
<!-- Consider Model 1 with $\boldsymbol{\theta}=[\boldsymbol{u}_1,...,\boldsymbol{u}_L,\sigma^2_{L+1},...,\sigma^2_J]$. We say that $\sigma^2_1,...,\sigma^2_J$ match their intuitive interpretation if -->
<!-- \begin{itemize} -->
<!--     \item for fixed effects $(j=1....,L)$: -->
<!--     \begin{gather}\label{eq:fe_int_req}  -->
<!--          \sigma^2_j=E_{\boldsymbol{u}_j}\{Var_{X_j}[f_j(X_j)|\boldsymbol{u}_j]|\sigma^2_j\} -->
<!--     \end{gather} -->
<!--      \item for random effects $(j=L+1....,J)$: -->
<!--     \begin{gather}\label{eq:re_int_req}       -->
<!--     \sigma^2_j= Var_{X_j,\boldsymbol{u}_j}[f_j(X_j)|\sigma^2_j] -->
<!--     \end{gather}  -->
<!-- \end{itemize} -->
<!-- \end{definition} -->
<!-- Definition \ref{def:int_int} answers the concern raised in \cite{F20} about the introduction of fixed effects and is consistent with the authors' suggestion of considering the concept of \textit{explained variance}.  On the other hand, the definition for random effects is coherent with the R2D2 approach.  -->

<!-- Inspired by the work of \cite{SR14}, we propose a standardization procedure that ensures that the conditions of Definition \ref{def:int_int} are satisfied for all effects of Model 1. -->

<!-- \begin{proposition}[Standardization procedure]\label{prop:standardization} -->
<!-- Consider Model 1. Definition \ref{def:int_int} is satisfied after the application of the following two steps. -->
<!-- \begin{enumerate} -->
<!-- \item \textbf{0-mean constraint}. Each fixed effect $(j=1,...,L)$ is constrained such that: -->
<!-- \begin{equation} -->
<!-- \label{eq:0mean_constraint} -->
<!-- \begin{aligned} -->
<!--  E_{X_j}[f_j(X_j)]=0 -->
<!-- \end{aligned} -->
<!-- \end{equation} -->
<!-- \item \textbf{Scaling}. Each effect $j=1,...,J$ is replaced by $\widetilde{f}_j(X_j)=\dfrac{f_j(X_j)}{\sqrt{C}_j}$ where:  -->
<!-- \begin{equation} -->
<!-- \label{eq:scaling_constant} -->
<!-- \begin{aligned} -->
<!--   C_j&= Var_{X_j,\boldsymbol{u}_j}[f_j(X_j)|\sigma^2_j=1]\\ -->
<!--   &=E_{X_j}\left[\boldsymbol{D}^T_j(X_j)\boldsymbol{Q}_j^*\boldsymbol{D}_j(X_j)\right]\\ -->
<!-- \end{aligned} -->
<!-- \end{equation} -->
<!-- \end{enumerate}  -->
<!-- \end{proposition} -->

<!-- Once it has been proven that the $\sigma^2_j$ parameters match their intuitive interpretation, this result can be used to derive expressions for the VP  parameters that are interpretable for the user. -->

<!-- The procedure is called \textit{standardization} as it can be proven that classical standardization is a special case that arises in the case of linear effects. -->

